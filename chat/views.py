from django.http import JsonResponse
from django.http import StreamingHttpResponse
from django.views.decorators.csrf import csrf_exempt
import openai
import json
from django.conf import settings
from drf_yasg.utils import swagger_auto_schema
from drf_yasg import openapi
from rest_framework.decorators import api_view
from rest_framework.response import Response
import threading
import queue
# --- LangChain ê´€ë ¨ ì„í¬íŠ¸ ---
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler

# ì˜ˆì‹œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ì‹¤ì œ ì„œë¹„ìŠ¤ì— ë§ê²Œ êµì²´)
docs = [
    "ì‘í’ˆ AëŠ” 19ì„¸ê¸° ìœ ëŸ½ì˜ ëŒ€í‘œì ì¸ ì¸ìƒì£¼ì˜ ì‘í’ˆìœ¼ë¡œ, ë¹›ê³¼ ìƒ‰ì±„ì˜ ì¡°í™”ê°€ ë‹ë³´ì…ë‹ˆë‹¤.",
    "ì‘í’ˆ BëŠ” í˜„ëŒ€ ë¯¸ìˆ ì˜ ê²½í–¥ì„ ë³´ì—¬ì£¼ë©°, ê°ì •ì˜ í‘œí˜„ê³¼ ì¶”ìƒì  í˜•ìƒì´ íŠ¹ì§•ì…ë‹ˆë‹¤.",
]

# OpenAI Embeddingsë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• (í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)
vectorstore = FAISS.from_texts(docs, embeddings)

# Swagger ìš”ì²­ ë° ì‘ë‹µ ì •ì˜
message_param = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        "message": openapi.Schema(type=openapi.TYPE_STRING, description="User message")
    },
    required=["message"],
)

response_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        "response": openapi.Schema(type=openapi.TYPE_STRING, description="AI response")
    },
)

@swagger_auto_schema(
    method="post",
    request_body=message_param,
    responses={200: response_schema},
    operation_description="GPT-4oì™€ ëŒ€í™”í•˜ë©° SSEë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.",
)
@api_view(["POST"])
@csrf_exempt
def chat_view(request):
    """
    ì‘í’ˆ ë„ìŠ¨íŠ¸ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AI ì±—ë´‡.
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ 30ë‹¨ì–´ ì´ë‚´ì˜ ì„¤ëª…ì„ ë°˜í™˜í•˜ë©°,
    GPTì˜ ì‘ë‹µì„ SSEë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    try:
        data = json.loads(request.body)
        user_message = data.get("message", "")
        if not user_message:
            return StreamingHttpResponse(
                "data: {\"error\": \"Message cannot be empty\"}\n\n",
                content_type="text/event-stream",
                status=400
            )

        def event_stream(user_message):
            # í† í°ì„ ì „ë‹¬í•  í ìƒì„±
            token_queue = queue.Queue()

            # SSE ì „ì†¡ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ ì½œë°± í•¸ë“¤ëŸ¬
            class SSECallbackHandler(BaseCallbackHandler):
                def on_llm_new_token(self, token: str, **kwargs) -> None:
                    token_queue.put(token)

            callback_handler = SSECallbackHandler()
            callback_manager = CallbackManager([callback_handler])

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            prompt_template = PromptTemplate(
                template=(
                    "ë‹¹ì‹ ì€ ì‘í’ˆ ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ 30ë‹¨ì–´ ì´ë‚´ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\n"
                    "Context: {context}\n"
                    "Question: {question}\n"
                    "Answer:"
                ),
                input_variables=["context", "question"]
            )

            # ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ í™œì„±í™”í•œ LLM ì´ˆê¸°í™”
            llm = ChatOpenAI(
                model_name="gpt-4o-mini",   # ğŸ”¥ gpt-4o â†’ gpt-4o-mini ë¡œ ë³€ê²½
                temperature=0,
                openai_api_key=settings.OPENAI_API_KEY,
                streaming=True,
                callback_manager=callback_manager
            )

            # RetrievalQA ì²´ì¸ ìƒì„±
            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(),
                chain_type_kwargs={"prompt": prompt_template}
            )

            # ì²´ì¸ ì‹¤í–‰ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìˆ˜í–‰
            result = {}

            def run_chain():
                nonlocal result
                try:
                    result = qa_chain({"query": user_message})
                except Exception as e:
                    token_queue.put(f"[error] {str(e)}")
                finally:
                    # ì²´ì¸ ì‹¤í–‰ ì™„ë£Œ í›„, sentinel ê°’ì„ ë„£ì–´ ë°˜ë³µë¬¸ ì¢…ë£Œ ì‹ í˜¸ ì „ë‹¬
                    token_queue.put(None)

            thread = threading.Thread(target=run_chain)
            thread.start()

            # í† í° íì—ì„œ í•˜ë‚˜ì”© ê°€ì ¸ì™€ SSE ì´ë²¤íŠ¸ë¡œ ì „ì†¡
            while True:
                token = token_queue.get()
                if token is None:
                    break
                yield f"data: {token}\n\n"

            thread.join()

        response = StreamingHttpResponse(event_stream(user_message), content_type="text/event-stream")
        response["Cache-Control"] = "no-cache"
        return response

    except Exception as e:
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì „ ë°œìƒí•œ ì˜¤ë¥˜ ì²˜ë¦¬
        return StreamingHttpResponse(
            f"data: {{\"error\": \"{str(e)}\"}}\n\n",
            content_type="text/event-stream",
            status=500
        )