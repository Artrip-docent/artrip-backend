from django.shortcuts import render, redirect
from django.contrib import messages
from .models import Exhibition, Document, Gallery
from .forms import ExhibitionForm, DocumentForm, GalleryForm
from django.http import JsonResponse
from django.http import StreamingHttpResponse
from django.views.decorators.csrf import csrf_exempt
import openai
import json
from django.conf import settings
from drf_yasg.utils import swagger_auto_schema
from drf_yasg import openapi
from rest_framework.decorators import api_view
from rest_framework.response import Response
import threading
import queue
import pinecone
# --- LangChain ê´€ë ¨ ì„í¬íŠ¸ ---
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
from pinecone import Pinecone, ServerlessSpec  # Pinecone í´ë¼ì´ì–¸íŠ¸ìš©
from langchain_pinecone import PineconeVectorStore  # LangChain VectorStoreìš©

# OpenAI Embeddingsë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• (í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
embeddings = OpenAIEmbeddings(openai_api_key=settings.OPENAI_API_KEY)

PINECONE_API_KEY = settings.PINECONE_API_KEY  # .envì— ì„¤ì •í•œ ê°’
PINECONE_ENV = settings.PINECONE_ENV          # ì˜ˆ: "us-east1-aws" ë˜ëŠ” "us-east-1" (Pinecone ëŒ€ì‹œë³´ë“œì— ë§ì¶°)
INDEX_NAME = settings.PINECONE_INDEX_NAME      # ì˜ˆ: "artwork-index"
# Pinecone ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
pc = Pinecone(api_key=PINECONE_API_KEY)

# ê¸°ì¡´ ì¸ë±ìŠ¤ ëª©ë¡ í™•ì¸ (ê° ì¸ë±ìŠ¤ ê°ì²´ì˜ name ì†ì„±ì„ ì‚¬ìš©)
existing_indexes = [idx.name for idx in pc.list_indexes()]
if INDEX_NAME not in existing_indexes:
    pc.create_index(
        name=INDEX_NAME,
        dimension=embeddings.embedding_dim,  # ì˜ˆ: 1536
        metric='cosine',  # í•„ìš”ì— ë”°ë¼ 'euclidean' ë˜ëŠ” 'dotproduct' ë“± ì„ íƒ
        spec=ServerlessSpec(
            cloud='aws',
            region=PINECONE_ENV
        )
    )
pinecone_index = pc.Index(INDEX_NAME)
index_stats = pinecone_index.describe_index_stats()
print(f"ğŸ“Š í˜„ì¬ Pinecone ì¸ë±ìŠ¤ ìƒíƒœ: {index_stats}")
vector_ids = pinecone_index.describe_index_stats().get('namespaces', {}).get('', {}).get('vector_count', 0)
print(f"ğŸ“Ší˜„ì¬ Pineconeì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {vector_ids}")
# Swagger ìš”ì²­ ë° ì‘ë‹µ ì •ì˜
message_param = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        "message": openapi.Schema(type=openapi.TYPE_STRING, description="User message"),
        "exhibition_id": openapi.Schema(type=openapi.TYPE_INTEGER, description="Exhibition ID (optional)"),
    },
    required=["message"],  # exhibition_idëŠ” ì„ íƒ ì‚¬í•­ì´ë©´ requiredì— ë„£ì§€ ì•ŠìŒ
)

response_schema = openapi.Schema(
    type=openapi.TYPE_OBJECT,
    properties={
        "response": openapi.Schema(type=openapi.TYPE_STRING, description="AI response")
    },
)

@swagger_auto_schema(
    method="post",
    request_body=message_param,
    responses={200: response_schema},
    operation_description="GPT-4o-miniì™€ ëŒ€í™”í•˜ë©° SSEë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.",
)
@api_view(["POST"])
@csrf_exempt
def chat_view(request):
    """
    ì‘í’ˆ ë„ìŠ¨íŠ¸ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AI ì±—ë´‡.
    ìš”ì²­ JSONì— 'message'ì™€ (ì„ íƒì ìœ¼ë¡œ) 'exhibition_id'ë¥¼ í¬í•¨í•˜ì—¬,
    exhibition_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì „ì‹œíšŒì˜ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì—†ìœ¼ë©´ GPT ì‘ë‹µë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        data = json.loads(request.body)
        user_message = data.get("message", "")
        exhibition_id = data.get("exhibition_id")  # ì „ì‹œíšŒ ID

        if not user_message:
            return StreamingHttpResponse(
                "data: {\"error\": \"Message cannot be empty\"}\n\n",
                content_type="text/event-stream",
                status=400
            )

        def event_stream(user_message):
            token_queue = queue.Queue()

            # ì½œë°± í•¸ë“¤ëŸ¬: í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ token_queueë¡œ ì „ë‹¬
            class SSECallbackHandler(BaseCallbackHandler):
                def on_llm_new_token(self, token: str, **kwargs) -> None:
                    token_queue.put(token)

            callback_handler = SSECallbackHandler()
            callback_manager = CallbackManager([callback_handler])

            # ê³µí†µ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt_template = PromptTemplate(
                template=(
                    "ë‹¹ì‹ ì€ ì‘í’ˆ ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ 30ë‹¨ì–´ ì´ë‚´ë¡œ ì¹œì ˆí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                    "{context}"
                    "Question: {question}\n"
                    "Answer:"
                ),
                input_variables=["context", "question"]
            )

            llm = ChatOpenAI(
                model_name="gpt-4o-mini",
                temperature=0,
                openai_api_key=settings.OPENAI_API_KEY,
                streaming=True,
                callback_manager=callback_manager
            )
            pinecone_vectorstore = PineconeVectorStore(
                index=pinecone_index,  # âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ì—ì„œ ìƒì„±í•œ Index ê°ì²´
                embedding=embeddings,  # âœ… OpenAI Embeddings ê°ì²´ ì „ë‹¬
                text_key="text",  # âœ… ì €ì¥ëœ í…ìŠ¤íŠ¸ í‚¤ (ì˜ˆ: "text" í•„ë“œ)
                namespace="",  # âœ… í•„ìš”í•œ ê²½ìš° ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
            )
            if exhibition_id:
                # exhibition_idê°€ ìˆëŠ” ê²½ìš°: RetrievalQA ì²´ì¸ ì‚¬ìš©

                retriever = pinecone_vectorstore.as_retriever(search_kwargs={
                    "filter": {"exhibition_id": exhibition_id}
                })
                # âœ… Pineconeì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸
                search_results = retriever.invoke(user_message)
                print(f"ğŸ” Exhibition {exhibition_id}ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼:")
                for doc in search_results:
                    print(doc.metadata)  # âœ… ë©”íƒ€ë°ì´í„° í™•ì¸
                    print(doc.page_content)  # âœ… ì €ì¥ëœ í…ìŠ¤íŠ¸ í™•ì¸
                qa_chain = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=retriever,
                    chain_type_kwargs={"prompt": prompt_template}
                )

                def run_chain():
                    try:
                        # ì´í•˜ ë¡œì§ì—ì„œ í† í°ì€ callback_managerë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì „ì†¡
                        qa_chain.invoke({"query": user_message})
                    except Exception as e:
                        token_queue.put(f"[error] {str(e)}")
                    finally:
                        token_queue.put(None)

                thread = threading.Thread(target=run_chain)
                thread.start()

            else:
                # exhibition_idê°€ ì—†ëŠ” ê²½ìš°: RetrievalQA ì—†ì´ ë°”ë¡œ LLM í˜¸ì¶œ
                def run_chain():
                    try:
                        # ì•„ë˜ í•œ ì¤„ì´ í•µì‹¬: llm(...) í˜¸ì¶œ ì‹œ streaming=True ì´ë¯€ë¡œ
                        # on_llm_new_token ì½œë°±ì´ ê³„ì†í•´ì„œ token_queueì— í† í°ì„ ë„£ì–´ì¤ë‹ˆë‹¤.
                        llm.invoke(prompt_template.format(context="", question=user_message))
                    except Exception as e:
                        token_queue.put(f"[error] {str(e)}")
                    finally:
                        token_queue.put(None)

                thread = threading.Thread(target=run_chain)
                thread.start()

            # token_queueì— ë“¤ì–´ì˜¤ëŠ” í† í°ì„ SSE í˜•íƒœë¡œ yield
            while True:
                token = token_queue.get()
                if token is None:
                    break
                yield f"data: {token}\n\n"
            thread.join()

        response = StreamingHttpResponse(event_stream(user_message), content_type="text/event-stream")
        response["Cache-Control"] = "no-cache"
        return response

    except Exception as e:
        return StreamingHttpResponse(
            f"data: {{\"error\": \"{str(e)}\"}}\n\n",
            content_type="text/event-stream",
            status=500
        )

def vectorize_text(text):
    # ì‹¤ì œ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¡œ ëŒ€ì²´í•˜ì„¸ìš”.
    return [0.0] * 768

def add_to_vector_db(document, content):
    """
    ì—…ë¡œë“œëœ ë¬¸ì„œ(document)ì˜ í…ìŠ¤íŠ¸(content)ë¥¼ ì„ë² ë”©í•˜ì—¬ Pinecone ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    ë¬¸ì„œì—ëŠ” exhibition_idë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ì €ì¥í•˜ì—¬ ì „ì‹œíšŒë³„ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    """
    # OpenAI ì„ë² ë”©ì„ í†µí•´ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    vector = embeddings.embed_query(content)
    # ê° ë¬¸ì„œì˜ ê³ ìœ  id (ë¬¸ì„œ ëª¨ë¸ì˜ pkë¥¼ ì‚¬ìš©)
    vector_id = f"doc_{document.id}"
    # ë©”íƒ€ë°ì´í„°ì— ì „ì‹œíšŒ idì™€ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨ (í•„í„°ë§ì— ì‚¬ìš©)
    metadata = {
        "exhibition_id": str(document.exhibition.id),
        "text": content,
    }
    # âœ… ë””ë²„ê¹…: ì €ì¥í•  ë°ì´í„° ì¶œë ¥
    print(f"ğŸ”¹ ì €ì¥í•  ë²¡í„° ID: {vector_id}")
    print(f"ğŸ”¹ ì €ì¥í•  ë²¡í„° í¬ê¸°: {len(vector)}")
    print(f"ğŸ”¹ ì €ì¥í•  ë©”íƒ€ë°ì´í„°: {metadata}")
    # Pinecone ì¸ë±ìŠ¤ì— ì—…ì„œíŠ¸
    pinecone_index.upsert(vectors=[(vector_id, vector, metadata)])
    print(f"ë¬¸ì„œ {document.id} (ì „ì‹œíšŒ {document.exhibition.id})ê°€ Pineconeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def admin_page(request):
    gallery_form = GalleryForm(prefix="gal")
    exhibition_form = ExhibitionForm(prefix="exh")
    document_form = DocumentForm(prefix="doc")

    if request.method == "POST":
        if "submit_document" in request.POST:
            document_form = DocumentForm(request.POST, request.FILES, prefix="doc")
            if document_form.is_valid():
                uploaded_file = document_form.cleaned_data["file"]
                if not uploaded_file:
                    messages.error(request, "íŒŒì¼ì„ ì„ íƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return redirect("admin_page")

                content = uploaded_file.read().decode("utf-8")
                doc_instance = document_form.save(commit=False)
                doc_instance.content = content
                doc_instance.save()

                # âœ… Pinecone ë²¡í„° DB ì¶”ê°€
                add_to_vector_db(doc_instance, content)

                messages.success(request, "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return redirect("admin_page")

    exhibitions = Exhibition.objects.all().order_by("-id")
    documents = Document.objects.all().order_by("-created_at")
    galleries = Gallery.objects.all()

    context = {
        "gallery_form": gallery_form,
        "exhibition_form": exhibition_form,
        "document_form": document_form,
        "exhibitions": exhibitions,
        "documents": documents,
        "galleries": galleries,
    }
    return render(request, "chat/admin_page.html", context)